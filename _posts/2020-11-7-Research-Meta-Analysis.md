---
layout: post
title: Research Meta Analysis
subtitle: Lab 2
# gh-repo: daattali/beautiful-jekyll
# gh-badge: [star, fork, follow]
# tags: [test]
# comments: true
author: Henry Greenhut
---
Author: Henry Greenhut
<br/>

## Are chess players smarter than non-chess players? A meta-analysis. ##

Giovanni Sala, Alexander P. Burgoyne, Brooke N. Macnamara, David Z. Hambrick, Guillermo Campitelli, Fernand Gobet,
Checking the “Academic Selection” argument. Chess players outperform non-chess players in cognitive skills related to intelligence: A meta-analysis,
Intelligence,
Volume 61,
2017,
Pages 130-139,
ISSN 0160-2896,
https://doi.org/10.1016/j.intell.2017.01.013.
[Research Paper](https://reader.elsevier.com/reader/sd/pii/S0160289616301635?token=86F16125186C1BCBE67F2AC68C7B2045B53E7FC4460912DA075BB7907886111FE9D2599887B22BDF3B42219A463205F7&originRegion=us-east-1&originCreation=20221106213613)

---
1. What are the null and alternative hypotheses?
<br/>
Null hypothesis: There is no correlation between playing chess and intelligence.
<br/>
Alternative hypotheses: Chess players are generally more or less intelligent than non-chess players.

2. Who is collecting and analyzing this data?
<br/>
This study is a meta-analysis of seven other studies which compare the results of some intelligence test between chess-players and non-chess players. The meta-analysis is done by six university graduates: Giovanni Sala, Alexander P. Burgoyne, Brooke N. Macnamara, David Z. Hambrick, Guillermo Campitelli, and Fernand Gobet. They reference the studies by the name of their primary researchers, such as Doll and Mayr, but it is unclear in those studies who physically collects the data. One [study](https://www.jstor.org/stable/10.5406/amerjpsyc.124.2.0213?seq=1#metadata_info_tab_contents), for instance, mentions that chess players were recruited from a local chess club and brought into a lab room, but does not name who administered the test.
<br/>
<br/>
3. What datasets does this study reference or use? Are these datasets available to the public?
<br/>
This paper analyses seven studies conducted between 1987 and 2016. Each of these studies created their own data by administering intelligence tests to chess players & non-chess players. All of the data created is available to the public, although some behind a paywall. For one study, they emailed the researcher and recieved unpublished data, although it is published now. The meta-analysis is also transparent about any manipulation done to the data.
<br/>

4. Why are they interested in this data?
<br/>
The overarching question that drives this meta-analysis is whether experts are inherently more intellegent than non-experts; or, whether experts tend to score higher on intelligence tests simply because to become an expert, you often first have to do well on tests to recieve training in the first place (ex. University students have to score well on standardized tests to be admitted to schools and then become experts). Chess is the chosen subject-matter because there are no intelligence tests you have to pass to learn chess & become an expert.
<br/>
<br/>
5. What data is being recorded? What data might be left out?
<br/>
The criteria to be included in the meta-analysis was that the study must compare chess-players and non-chess players, must administer some test of intelligence, and must contain enough data to calculate an effect size. The chess players ranged from middle schoolers to chess masters. Only seven studies were picked out of thousands. Information such as how long the participants have been playing chess for and how much they play is left out.
<br/>
<br/>
6. What evidence did they present to back up their conclusions?
<br/>
The meta-analysis concluded that chess players' overall cognitive ability is higher than their same-aged non-chess playing counterparts, with a p value below 0.001. They took into account 7 different studies done on different skill levels with different intelligence tests administered. The different studies used different tests of intelligence. However, the researchers behind the meta-analysis claim that the results are comparable by referencing a separate study which proved that there is the strong correlation between the different tests used.
<br/>
<br/>
7. How was this study funded?
Funding for the meta-analysis and for the studies the meta-analysis cites is not clear. The study conducted by Campitelli and Labollita, for example, mentions that they granted a $12 compensation to the chess players they studied, but do not cite any funding.
<br/>
<br/>
8. Do you think publish or perish had an effect on this study?
<br/>
I think "publish or perish" applies to all scientific papers to some extent. The article is published to Elsevier, which has many guidelines for publishing, such as "Do your findings advance understanding in a
specific research field?" which would encourage novel findings. However, the meta-analysis spends significant time on meta-analytic models that investigate whether these studies are influenced by publication bias & outliers. The meta-analysis also describes the limitations of the study -- the main one being the limited data collected in the research studies they cite.
The p-value for the meta-analysis determined was less than 0.001. However, they describe how removing a study would greatly change the p-value. As they picked between 2287 studies, the p-value likely would be vastly different depending on which studies they evaluate.
I do think that this is a valuable and carefully-done meta-analysis.
<br/>